{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dac9739",
   "metadata": {},
   "source": [
    "# Vision Transformers for Edge Devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba293ea",
   "metadata": {},
   "source": [
    "MobileViT core step for rearranging convolution2d activation maps into (B, T, C) format for use with transformers.\n",
    "\n",
    "The key takeaway here is that rather than merging the patches into the channel dimension i.e `(B, C, H, W) -> (B, (h*w), (pw*ph*C))` to form (B,T,C) for standard ViTs. \n",
    "\n",
    "MobileViTs merge the patches into the batch dimension i.e `(B, C, H, W) -> (B*pw*ph, (h*w), C)` to form (B,T,C). This allows each patch to attend to other patches using the `C` data obtained from convolutional layers rather than ViTs which create their `C` data from the raw pixel values. \n",
    "\n",
    "**It's a powerful 2 step process where it parallelises the patch to patch attention across the entire batch dimension and allows the model to learn from the convolutional features rather than raw pixel values.**\n",
    "\n",
    "$$X_{G}(p)=\\text{Transformer}(X_{U}(p))$$\n",
    "\n",
    "Here, the transformer is applied independently for each $p\\in\\{1,\\cdot\\cdot\\cdot,P\\}$. By reshaping to an effective batch size of $B \\times P$, standard PyTorch transformer layers can process all $P$ relative pixel locations across all $B$ images entirely in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82ea4ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor = torch.Size([8, 64, 128, 128])\n",
      "------------------------------\n",
      "Unfolded tensor = torch.Size([128, 1024, 64])\n",
      "Folded tensor = torch.Size([8, 64, 128, 128])\n",
      "------------------------------\n",
      "Unfolded tensor = torch.Size([128, 1024, 64])\n",
      "Folded tensor = torch.Size([8, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "H, W = 128, 128\n",
    "C = 64\n",
    "B = 8\n",
    "\n",
    "x = torch.randn(B, C, H, W)\n",
    "print(f\"Original tensor = {x.shape}\")  # (B, C, H, W)\n",
    "\n",
    "print(\"---\" * 10)\n",
    "\n",
    "# UNFOLD\n",
    "# Merge the patches into the batch dimension, and rearrange to (B, T, C) format for use with transformers.\n",
    "ph, pw = 4, 4\n",
    "# re-arrange from (B, C, (h ph), (w pw)) -> (B, C, N, P)\n",
    "tx = rearrange(x, 'b c (h ph) (w pw) -> b c (h w) (ph pw)', ph=ph, pw=pw)\n",
    "tx = rearrange(tx, 'B C N P -> (B P) N C')  # (B, T, C) where P is the patch size (ph*pw), and N is the number of patches (H//ph * W//pw) and C is the number of channels.\n",
    "print(f\"Unfolded tensor = {tx.shape}\")   # (BP, N, C) -> (B*pw*ph, h*w, C) -> (B, T, C)\n",
    "# NB: For plain ViT, we would just do rearrange(x, 'b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=ph, pw=pw) to get (B, T, C) directly without merging the patch dimension into the batch dimension.\n",
    "\n",
    "\n",
    "# FOLD\n",
    "# re-arrange from (BP, N, C) OR (B, T, C) -> (B, C, (h ph), (w pw))\n",
    "x2 = rearrange(tx, '(b p) (h w) c -> b c (h w) p', h=H//ph, w=W//pw, p=ph*pw)\n",
    "x2 = rearrange(x2, 'b c (h w) (ph pw) -> b c (h ph) (w pw)', ph=ph, pw=pw, h=H//ph)\n",
    "print(f\"Folded tensor = {x2.shape}\")  # (B, C, H, W)\n",
    "\n",
    "print(\"---\" * 10)\n",
    "\n",
    "# The above was done for clarity, but we can also do it in one step:\n",
    "# UNFOLD: Single-step rearrange to (B*P, N, C)\n",
    "# B*P = b * ph * pw\n",
    "# N = h * w (where h = H//ph and w = W//pw)\n",
    "tx = rearrange(x, 'b c (h ph) (w pw) -> (b ph pw) (h w) c', ph=ph, pw=pw)\n",
    "print(f\"Unfolded tensor = {tx.shape}\") \n",
    "\n",
    "# FOLD: Single-step reverse rearrange back to (B, C, H, W)\n",
    "x2 = rearrange(tx, '(b ph pw) (h w) c -> b c (h ph) (w pw)', b=B, ph=ph, pw=pw, h=H//ph, w=W//pw)\n",
    "print(f\"Folded tensor = {x2.shape}\")\n",
    "\n",
    "# Sanity check to ensure the math maps perfectly\n",
    "assert torch.allclose(x, x2), \"Folded tensor does not match the original!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile-vits (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
